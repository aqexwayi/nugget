- Active TODO items
 - autogenerate mclosure macros, and closure types??
   no, cannot do that because closure arity is embedded in tags, GC, etc.

   short term can just add more for eval.scm, BUT...

   longer term, will probably need a more robust solution to these
   n-ary problems, other than generating macros. how will the evaluator handle
   all this stuff? will adding an arity parameter help fix any of it??
   
   need to investigate implementing n-arity closures somehow. I suppose the closure would have to
   include num_args and an area for those args. tbd on how to implement this, especially while
   preventing memory leaks (IE, cannot just use malloc because GC would not be able to reclaim lost objs)

try experimentation on a branch. for example:

  typedef struct {
    tag_type tag; 
    function_type fn; 
    int num_elt;
    object *elts;
  } closure_type;

could alloca the elt's. need to be careful how this is handled, especially during GC.
maybe throw an error if num_elt exceeds a certain number (100?)

- Add eval support. 
    - moving forward with meta-circular evaluator from SICP. Let's try to add more cases and code, and see how far it can be pushed.
    - also want to try integrating it with trans somehow. IE, if eval is a free var, then add eval code to the compiled program.

Original notes: Will probably also require apply, read, etc. will probably be necessary to write interpreter in CPS style - see notes. One idea - parse and compile input to scheme expressions, then call apply. could that work? should also see chapter 6 of lisp in small pieces.

- Improvements to symbol handling, including printing of symbols with invalid C chars, like 'hello-world
- Implement ER-macros using eval, in scheme. with this in place, could implement basic macros using ER and replace "desugar". presumably syntax rules could be implemented this way as well, but that can wait for a later time
- Pass port along with emit procedures, to allow the scheme code to write to an output file (IE, c file)?? Or is with-output-file good enough? unfortunately that is not in husk yet so it makes boostrapping harder
- Test with nested variables of the same name (EG: 'x'). does it work, or do we need alpha conversion to prevent
  collisions? I suspect without changes, that lexical scoping will not be preserved.
- Add more numeric support, and doubles 
- WRT set! support, and mutable variables:
  - set aggressive GC, and see if there are any problems with data being lost
    need to do this with a more complicated example, though
- Could add other scheme library functions to the compiled prog just
  like call/cc. alternatively could compile them into a library somewhere
  for inclusion.
- define - can this with with mutable variable elimination, or does it require C globals (per cboyer)? Are there special cases for top-level? If cells can be used for vars, do we need to keep track of the roots to prevent invalid GC? lots of questions here
- Question about closures and continuations:
 Presumably every function will recieve a closure. Do we have to differentiate between continuation (which every
 function must have) and closure (which can be empty if no fv)? right now the MTA runtime combines the two by
 having an fn argument to each closure. Is that OK?

 FWIW, chicken passes the following to generated C funcs:
   - number of args
   - env (the closure from caller)
   - continuation (function to call into)
   - actual args

- may be necessary to specify arity of functions in call to apply
- GC - notes from: http://www.more-magic.net/posts/internals-gc.html

 JAE - Good notes here about mutations (use write barrier to keep track of changes, EG: vector-set!). remember changes so they can be handled properly during next GC

 JAE - worth considering having immediate values, which take advantage of fact that lowest bit is 0 for pointers (EG: boxed values)?

 JAE - Important point, that the heap must be reallocated during a major GC if there is too much data in the stack / old heap. Considered this but not sure if cyclone's GC does that right now:

The smart reader might have noticed a small problem here: what if the amount of garbage cleaned up is less than the data on the stack? Then, the stack data can't be copied to the new heap because it simply is too small. Well, this is when a third GC mode is triggered: a reallocating GC. This causes a new heap to be allocated, twice as big as the current heap. This is also split in from- and tospace. Then, Cheney's algorithm is performed on the old heap's fromspace, using one half of the new heap as tospace. When it's finished, the new tospace is called fromspace, and the other half of the new heap is called tospace. Then, the old heap is de-allocated.

- Just a thought: if this ever became self-hosting, could distribute compiled C files
