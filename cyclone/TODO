Add stuff required for eval:
 - Optimizations - there are massive performance problems in compiling eval.scm!!!
   it might be better for (define) statements to create GC roots, rather than attempting to pass
   all defines around in closures. that should minimize closure size and the amount of data each function
   needs to pass around (IE: funcall80)

Here are pure lines-of-code metrics:
Phase   Last line   First line  LOC % LOC
input   206 13  193 0.31%
expand  601 212 389 0.63%
alpha   1253    607 646 1.05%
CPS 2586    1259    1327    2.15%
wrap-mut    4151    2676    1475    2.39%
CC  29738   4157    25581   41.49%
C   61652   29743   31909   51.76%

should compare times in each phase. but this still avoids WHY the last two phases are taking so long.
is it just the sheer amount of code that is being generated?

notes about globals:
would have to occur after expand, since that could introduce more defines
want to take each top-level define and create a global for it
could filter top level into two categories - (define) expressions and other expressions, to be enclosed within a (begin)
could create a single list of these, eg: (define, define, ..., begin) and then map over it for the other phases
TBD: how to store defined functions? Do they become closures? how does that work?

 - C compiler errors:

eval.c: In function âlambda_240âeval.c:55232:1: error: âote_carândeclared (first use in this function)
eval.c:55232:1: note: each undeclared identifier is reported only once for each function it appears in
eval.c: In function âlambda_238âeval.c:55385:1: error: âote_cdrândeclared (first use in this function)
eval.c: In function âlambda_236âeval.c:55540:1: error: âote_consândeclared (first use in this function)
eval.c: In function âlambda_233âeval.c:55776:1: error: âimitive_null_127ândeclared (first use in this function)
eval.c: In function âlambda_140âeval.c:59542:1: error: âimitive_cadrândeclared (first use in this function)

 - String support
   issue is how to support strings themselves in memory. can add them directly to the string_type, but then apply won't work 
   because it could return an unknown number of bytes. on the other hand could use a separate data heap that is mirrored during GC.
   may need some extra buffer for that because technically it could overflow any time a new string is allocated, not just during
   function calls. but this would work for apply as well as everything else, I believe. obviously it makes GC a bit harder because
   there is another pair of heaps to deal with. but all that would be done is that strings from heap A would be copied to B during GC.
   GC would need to keep track of a pointer to each one. Sounds straightforward, but have to be careful of complications.
   Initial plan:
    - Add two "data" heap sections, and vars for each (head ptr, pos ptr [active only?], size)
    - Allocate string on active data heap via make_string
    - Initiate GC when stack exceeded or data heap under certain threshold
    - Need adequate extra space in data heap (100K? make config), since we only check it upon function call
    - Need to update GC to copy strings to other heap
    - Wait, this is broken if anything is pointing to one of these strings, since memory location changes upon GC!
      Is that a fatal issue? How to handle? could write string operations such that any operate on copies of
      strings rather than pointing to another string. not nearly as efficient but avoids this problem. could revisit
      other solutions down the road.
    - Anything else? Probably want to branch for this development, just in case there are complications

  COMPLICATION - only need to memcpy strings on data heap during a major collection. during a minor collection the strings are already where they need to be
  need to fully-implement this in the runtime by passing minor/major flag to transport

  TODO: trigger GC if data heap too low
  TODO: once this works but before moving all, consolidate all this in docs/strings.txt or such. would be useful to keep these notes


- Unit test improvements
  - concatenate all into one file when compiling / running
  - add assert functions, and actually test for equality
    otherwise it is too easy to miss failing test cases, unless they
    blow up the runtime

- in regard to native apply support for primitives
  - do we need to create a table of primitives, like husk?
    might allow for more efficient comparisons than the stupid string cmp's
    that are required if we use symbols to denote primitives (which also 
    breaks lexical scoping)

- Improved symbol storage
  as part of above, probably will need a more dynamic and accurate way to store symbols.
  for example, how to store the + symbol, how to differentiate #t and 't etc.
  perhaps could use a malloc'd table for this? do want the lookups to be fast - IE, integer (pointer) comparisons and NOT string comparisons (those are unacceptable for symbols)
- Improvements to symbol handling, including printing of symbols with invalid C chars, like 'hello-world
- Consoldate list of primitives in (prim?) and (c-compile-prim?). should also include other information such as number of args (or variable args), for error handling

- Notes on implementing variables

 * could maintain env's in the interpreted code, and perform operations there to lookup vars. If a lookup fails though, would have to then fall back to looking in the compiled code. The compiler would have to (only if eval is used) set aside data to allow a reference back to vars in this case. Also, this becomes tricky because a var may not even be used, so it might not be added to any closures. There may have to be special analysis done if eval is used.
 * chicken does this, obviously. could map var ==> rename (how to know?) and then look it up in a C-based symbol table 
 * lexical addressing (see chapter 5 of SICP) can be used to find a variable in recursive env's, so you can access it directly instead of having to recursively traverse environments.

- Add eval support. 
    - moving forward with meta-circular evaluator from SICP. Let's try to add more cases and code, and see how far it can be pushed.
    - also want to try integrating it with trans somehow. IE, if eval is a free var, then add eval code to the compiled program.

Original notes: Will probably also require apply, read, etc. will probably be necessary to write interpreter in CPS style - see notes. One idea - parse and compile input to scheme expressions, then call apply. could that work? should also see chapter 6 of lisp in small pieces.

- Integrate debug script into cyclone, so that by passing a specific command line arg, the compiler will output results of closure-conversion, prepended with the debug contents. That way the SCM code can be debugged independently of the compiled executable.

- What happens when a continuation is captured? assigned to a variable? applied?
  Should look into this a bit using cyclone and call/cc

- Implement ER-macros using eval, in scheme. with this in place, could implement basic macros using ER and replace "desugar". presumably syntax rules could be implemented this way as well, but that can wait for a later time
- Pass port along with emit procedures, to allow the scheme code to write to an output file (IE, c file)?? Or is with-output-file good enough? unfortunately that is not in husk yet so it makes boostrapping harder
- Add more numeric support, and doubles 
- WRT set! support, and mutable variables:
  - set aggressive GC, and see if there are any problems with data being lost
    need to do this with a more complicated example, though
- Could add other scheme library functions to the compiled prog just
  like call/cc. alternatively could compile them into a library somewhere
  for inclusion.
- define - can this with with mutable variable elimination, or does it require C globals (per cboyer)? Are there special cases for top-level? If cells can be used for vars, do we need to keep track of the roots to prevent invalid GC? lots of questions here
- Question about closures and continuations:
 Presumably every function will recieve a closure. Do we have to differentiate between continuation (which every
 function must have) and closure (which can be empty if no fv)? right now the MTA runtime combines the two by
 having an fn argument to each closure. Is that OK?

 FWIW, chicken passes the following to generated C funcs:
   - number of args
   - env (the closure from caller)
   - continuation (function to call into)
   - actual args

- may be necessary to specify arity of functions in call to apply
- GC - notes from: http://www.more-magic.net/posts/internals-gc.html

 JAE - Good notes here about mutations (use write barrier to keep track of changes, EG: vector-set!). remember changes so they can be handled properly during next GC:

 Another major oversight is the assumption that objects can only point from the stack into the heap. If Scheme was a purely functional language, this would be entirely accurate: new objects can refer to old objects, but there is no way that a preexisting object can be made to refer to a newly created object. For that, you need to support mutation.
 But Scheme does support mutation! So what happens when you use vector-set! to store a newly created, stack-allocated value in an old, heap-allocated vector? If we used the above algorithm, the newly created element would either be part of the live set and get copied, but the vector's pointer would not be updated, or it wouldn't be part of the live set and the object would be lost in the stack reset.
 The answer to this problem is also pretty simple: we add a so-called write barrier. Whenever a value is written to an object, it is remembered. Then, when performing a GC, these remembered values are considered to be part of the live set, just like the addresses in the saved call. This is also the reason CHICKEN always shows the number of mutations when you're asking for GC statistics: mutation may slow down a program because GCs might take longer.

 JAE - Important point, that the heap must be reallocated during a major GC if there is too much data in the stack / old heap. Considered this but not sure if cyclone's GC does that right now:

The smart reader might have noticed a small problem here: what if the amount of garbage cleaned up is less than the data on the stack? Then, the stack data can't be copied to the new heap because it simply is too small. Well, this is when a third GC mode is triggered: a reallocating GC. This causes a new heap to be allocated, twice as big as the current heap. This is also split in from- and tospace. Then, Cheney's algorithm is performed on the old heap's fromspace, using one half of the new heap as tospace. When it's finished, the new tospace is called fromspace, and the other half of the new heap is called tospace. Then, the old heap is de-allocated.

- Just a thought: if this ever became self-hosting, could distribute compiled C files
